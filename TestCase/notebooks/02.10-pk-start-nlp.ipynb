{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T08:39:43.815717Z",
     "start_time": "2024-10-06T08:39:36.778280Z"
    }
   },
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 11:39:40.689123: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-06 11:39:40.773704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 11:39:41.799018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:39:45.261067Z",
     "start_time": "2024-10-06T08:39:43.829062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
    "metric = load_metric(\"seqeval\")"
   ],
   "id": "f0c37be9413cb1a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/Programms/miniconda3/envs/blyat/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5051/3917191781.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:39:45.371270Z",
     "start_time": "2024-10-06T08:39:45.284791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_bio_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Strip any extra whitespace from the line\n",
    "            line = line.strip()\n",
    "\n",
    "            # If we encounter an empty line, we finish the current sentence\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "            else:\n",
    "                # Split the word and the label by space\n",
    "                word, tag = line.split()\n",
    "                sentence.append(word)\n",
    "                label.append(tag)\n",
    "\n",
    "        # Catch any final sentence that might not end with a newline\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the sentences and labels from the BIO file\n",
    "sentences, labels = read_bio_file('../src/output_bio.txt')\n",
    "\n",
    "# Create a dataset dictionary with 'text' and 'labels'\n",
    "dataset_dict = {\n",
    "    'text': sentences,\n",
    "    'labels': labels\n",
    "}\n",
    "\n",
    "# Convert the dictionary to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "print(dataset[0])"
   ],
   "id": "d3f8e8d2e96e869c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Rusitc', 'Two', 'Tone', 'Gathering', 'Table', 'and', 'Barstools', '-', 'Allwood', 'Furniture', 'Menu', 'Home', 'Catalog', 'All', 'Collections', 'Barstool', 'Collection', 'Dining', 'Collection', 'Pub', 'Collection', 'Amish', 'Collection', 'Sizing', 'Contact', 'Order', 'Form', 'Continue', 'Shopping', 'Your', 'Cart', 'is', 'Empty', 'SUBMIT', 'ORDERS', 'AND', 'PRODUCT', 'ISSUES', 'WITH', 'PICTURE', 'VIA', 'EMAIL:', 'support@allwoodfurn.com', 'Allwood', 'Furniture', 'Home', 'Catalog', '‚ñæ', 'All', 'Collections', 'Barstool', 'Collection', 'Dining', 'Collection', 'Pub', 'Collection', 'Amish', 'Collection', 'Sizing', 'Contact', 'Order', 'Form', 'Allwood', 'Furniture', 'Home', '/', 'Products', '/', 'Group', '#119', 'Group', '#119', '5600/5601', 'Rustic', 'Two', 'Tone', 'Gathering', 'Table', 'and', 'Barstools', 'Table:', '5600', '*', '42‚ÄùW', 'x', '60/78‚ÄùD', 'x', '36‚ÄùH', '*', 'Rustic', 'Two-Tone', 'Rectangular', 'Gathering', 'Table', 'w/Self-Storing', 'Butterfly', 'Leaf', 'Barstool:', '5601', '*', '24‚ÄùW', 'x', '24‚ÄùD', 'x', '42‚ÄùH', '*', 'Rustic', 'Two-Tone', 'Barstool', 'Request', 'a', 'quote', 'Share:', 'News', '&', 'Updates', 'Sign', 'up', 'to', 'get', 'the', 'latest', 'on', 'sales,', 'new', 'releases', 'and', 'more', '‚Ä¶', \"What's\", 'New', 'Welcome', 'to', 'Allwood', 'Furniture', 'Co.', 'Please', 'call', 'us', 'at', '800-320-1744', 'or', '651-917-9390', 'with', 'any', 'questions.', 'To', 'order', 'use', 'support@allwoodfurn.com', '¬©', '2024', 'Allwood', 'Furniture.', 'Ecommerce', 'Software', 'by', 'Shopify'], 'labels': ['B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:39:46.029540Z",
     "start_time": "2024-10-06T08:39:45.419878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    # Tokenize the input text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['text'],  # Tokenizing the 'text' field\n",
    "        padding='max_length',  # Pad to max length\n",
    "        truncation=True,  # Truncate sequences if they exceed max length\n",
    "        max_length=128,  # Set a max length for the sequences\n",
    "        is_split_into_words=True,  # Use this because we are dealing with word-level inputs\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word IDs for the tokenized inputs\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Special tokens like [CLS] or [SEP]\n",
    "                aligned_labels.append(-100)  # -100 will be ignored in the loss function\n",
    "            elif word_idx != previous_word_idx:  # First token of a word\n",
    "                aligned_labels.append(label[word_idx])\n",
    "            else:  # Subword token\n",
    "                aligned_labels.append(label[word_idx] if label_all_tokens else -100)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(aligned_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels  # Add aligned labels to the tokenized inputs\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create label-to-ID mapping\n",
    "label_list = ['O', 'B-PRODUCT', 'I-PRODUCT']  # Add all possible labels here\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "# Function to encode string labels into integers\n",
    "def encode_labels(labels, label_to_id):\n",
    "    return [[label_to_id[label] for label in sentence_labels] for sentence_labels in labels]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% val)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    dataset['text'], dataset['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create the train_data and val_data dictionaries\n",
    "train_data = {\n",
    "    'text': train_texts,\n",
    "    'labels': encode_labels(train_labels, label_to_id)\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'text': val_texts,\n",
    "    'labels': encode_labels(val_labels, label_to_id)\n",
    "}\n",
    "\n",
    "# Convert the data to Dataset format (train and validation sets)\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "train_dataset = train_dataset.map(lambda examples: tokenize_and_align_labels(examples, label_all_tokens=True), batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: tokenize_and_align_labels(examples, label_all_tokens=True), batched=True)\n"
   ],
   "id": "a0bea95c429b559c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "029e6e9c842e4f559b2094c73dd780f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19df9d1f994c4ef6860cf22b1b52db66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:41:16.053187Z",
     "start_time": "2024-10-06T08:39:46.047853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "d816ebc6a8d16d4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.411011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.403532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.3847005526224772, metrics={'train_runtime': 89.3423, 'train_samples_per_second': 2.686, 'train_steps_per_second': 0.168, 'total_flos': 15677947146240.0, 'train_loss': 0.3847005526224772, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:41:18.196516Z",
     "start_time": "2024-10-06T08:41:16.126565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model and evaluate\n",
    "predictions, labels, _ = trainer.predict(val_dataset)\n",
    "\n",
    "# Get the predicted class index for each token\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Flatten the true and predicted labels and remove the ignored index (-100)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    true_label = [label_list[l] for l in labels[i] if l != -100]  # True labels without ignored tokens\n",
    "    pred_label = [label_list[p] for j, p in enumerate(predictions[i]) if labels[i][j] != -100]  # Predictions without ignored tokens\n",
    "    true_labels.extend(true_label)  # Extend instead of append to flatten\n",
    "    pred_labels.extend(pred_label)  # Extend instead of append to flatten\n",
    "\n",
    "# Now print the classification report\n",
    "print(classification_report(true_labels, pred_labels, zero_division=0))"
   ],
   "id": "6e318f90d90d98ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-PRODUCT       0.00      0.00      0.00        52\n",
      "   I-PRODUCT       0.00      0.00      0.00       188\n",
      "           O       0.90      1.00      0.95      2198\n",
      "\n",
      "    accuracy                           0.90      2438\n",
      "   macro avg       0.30      0.33      0.32      2438\n",
      "weighted avg       0.81      0.90      0.85      2438\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:41:24.114120Z",
     "start_time": "2024-10-06T08:41:23.954470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = \"Ada Sofa is our new product.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs).logits\n",
    "\n",
    "# Get the predicted token labels\n",
    "predicted_token_class_ids = np.argmax(outputs.detach().numpy(), axis=-1)\n",
    "predicted_labels = [model.config.id2label[t] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "# Display the predictions\n",
    "print(predicted_labels)"
   ],
   "id": "1acfdb83a8251296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
